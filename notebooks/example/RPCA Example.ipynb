{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd, numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/adrian/Vadetis/vadetisweb/algorithms/rpca/loss.py\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from numbers import Number\n",
    "\n",
    "class MLoss:\n",
    "    \"\"\"\n",
    "    An abstract class for a loss function suitable for an M-estimator solution using IRLS. Has to\n",
    "    have a strict minimum at zero, be symmetric and positive definite, and increasing less than\n",
    "    square.\n",
    "\n",
    "    Programmatically, it has to implement a ``__call__(self, x)`` method, where `x` is a float, a\n",
    "    ``diff(self, x)`` method returning its derivative at ``x``, and (optionally) a ``weight(self,\n",
    "    x)=diff(self, x)/x``.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Evaluates the function at x.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        x : float\n",
    "            Point to evaluate the function at.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        y : float\n",
    "            Value at point x.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def diff(self, x):\n",
    "        \"\"\"\n",
    "        Evaluates the function's derivative at x.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        x : float\n",
    "            Point to evaluate the function derivative at.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        y : float\n",
    "            Derivative value at point x.\n",
    "        \"\"\"\n",
    "\n",
    "    def weight(self, x):\n",
    "        \"\"\"\n",
    "        Evaluates the weight function (:=f(x)/x) induced by the loss function at x.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        x : float\n",
    "            Point to evaluate the weight function at.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        y : float\n",
    "            Weight function value at point x.\n",
    "        \"\"\"\n",
    "        return self.diff(x) / float(x)\n",
    "\n",
    "class HuberLoss(MLoss):\n",
    "    \"\"\"\n",
    "    Huber loss function: \n",
    " \n",
    "    .. math::\n",
    "        f_\\\\delta (x) = \\\\begin{cases}\n",
    "        \\\\frac{x^2}{2},\\\\,\\\\,\\\\,&\\\\vert x\\\\vert\\\\leq\\\\delta,\\\\\\\\\n",
    "                \\\\delta\\\\left(\\\\vert x\\\\vert - \\\\frac{\\\\delta}{2} \\\\right),\\\\,\\\\,\\\\,&\\\\vert\n",
    "        x\\\\vert\\\\geq\\\\delta\n",
    "        \\\\end{cases}\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    delta : float >= 0\n",
    "        Delta parameter in function definition.\n",
    "    \"\"\"\n",
    "    def __init__(self, delta):\n",
    "        assert isinstance(delta, Number)\n",
    "        assert float(delta) >= 0, 'delta has to be non-negative.'\n",
    "\n",
    "        self.delta = float(delta)\n",
    "        self.delta_half_square = (self.delta ** 2) / 2.\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x_flt = float(x)\n",
    "        assert x_flt >= 0\n",
    "        if x_flt <= self.delta:\n",
    "            return (x_flt ** 2) / 2.\n",
    "        else:\n",
    "            return self.delta * x_flt - self.delta_half_square\n",
    "    \n",
    "    def diff(self, x):\n",
    "        pass\n",
    "\n",
    "    def weight(self, x):\n",
    "        x_flt = float(x)\n",
    "        assert x_flt >= 0\n",
    "        if x_flt <= self.delta:\n",
    "            return 1.\n",
    "        else:\n",
    "            return self.delta / x_flt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Vadetis/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.decomposition.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# %load /home/adrian/Vadetis/vadetisweb/algorithms/rpca/m_est_rpca.py\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition.base import _BasePCA\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "from scipy.sparse import issparse\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "\n",
    "class MRobustPCA(_BasePCA):\n",
    "    \"\"\"\n",
    "    An implementation of Robust PCA using M-estimator loss. A particular case with the Huber loss\n",
    "    function is described in the paper:\n",
    "\n",
    "        B.T. Polyak, M.V. Khlebnikov: Principal Component Analysis: Robust Variants, 2017.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    n_components: int or None\n",
    "        Number of components. If None, n_components = data dimensionality.\n",
    "\n",
    "    loss : MLossFunction object\n",
    "        Loss function to be optimized during fitting. See \"Loss functions\" for details.\n",
    "\n",
    "    model : string {'first', 'second'} (default 'first')\n",
    "        Statistical model to be used during fitting, according to the original method. First is\n",
    "        based on the iid noise assumption, second estimates the noise covariance structure but\n",
    "        assumes a single cluster center for all points. For more details, see the original paper.abs\n",
    "\n",
    "    eps : float >= 0, optional (default 1e-6)\n",
    "        Max relative error for the objective function optimised by IRLS. Used as a stopping\n",
    "        criterion.\n",
    "\n",
    "    max_iter : int >= 0, optional (default 100)\n",
    "        Max number of IRLS iterations performed during optimisation. Used as a stopping crietrion.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    components\\_ : array, [n_components, n_features]\n",
    "        Principal axes in feature space. The components are sorted by\n",
    "        ``explained_variance_``.\n",
    "\n",
    "    explained_variance\\_ : array, [n_components]\n",
    "        The amount of variance explained by each of the selected components.\n",
    "\n",
    "    explained_variance_ratio\\_ : array, [n_components]\n",
    "        Percentage of variance explained by each of the selected components.\n",
    "        If ``n_components`` is not set then all components are stored and the\n",
    "        sum of explained variances is equal to 1.0.\n",
    "\n",
    "    mean\\_ : array, [n_features]\n",
    "        Per-feature empirical mean, estimated from the training set.\n",
    "        Equal to `X.mean(axis=1)`.\n",
    "\n",
    "    n_components\\_ : int\n",
    "        The number of components. It equals the parameter\n",
    "        n_components, or n_features if n_components is None.    \n",
    "\n",
    "    weights\\_ : array, [n_samples]\n",
    "        Weights assigned to input samples during IRLS optimisation.\n",
    "\n",
    "    n_iterations\\_ : int\n",
    "        Number of iterations performed during fitting.\n",
    "\n",
    "    errors\\_ : array, [n_iterations\\_]\n",
    "        Errors achieved by the method on each iteration.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_components,\n",
    "                 loss,\n",
    "                 model='first',\n",
    "                 eps=1e-6,\n",
    "                 max_iter=100):\n",
    "        self.n_components = n_components\n",
    "        self.loss = loss\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.max_iter = max_iter\n",
    "        self.whiten = False # TODO: implement proper whitening\n",
    "\n",
    "    def fit(self, X, y=None, weights_init=None):\n",
    "        \"\"\"\n",
    "        Fit the model with X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples in the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        self._fit(X, weights_init)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "\n",
    "        Fits the model to X and returns a transformed version of X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples in the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new: array-like, shape (n_samples, n_components)\n",
    "            Transformed array.\n",
    "        \"\"\"\n",
    "        return super(MRobustPCA, self).fit_transform(X, y)\n",
    "\n",
    "    def _fit(self, X, weights_init=None):\n",
    "        # Raise an error for sparse input.\n",
    "        # This is more informative than the generic one raised by check_array.\n",
    "        if issparse(X):\n",
    "            raise TypeError('MRobustPCA does not support sparse input.')\n",
    "\n",
    "        X = check_array(X, dtype=[np.float64], ensure_2d=True,\n",
    "                        copy=True)\n",
    "\n",
    "        # Handle n_components==None\n",
    "        if self.n_components is None:\n",
    "            n_components = X.shape[1]\n",
    "        else:\n",
    "            n_components = self.n_components\n",
    "\n",
    "        return self._fit_model(X, n_components, weights_init)\n",
    "\n",
    "    def _fit_model(self, X, n_components, weights_init):\n",
    "        vectorized_loss = np.vectorize(self.loss.__call__)\n",
    "        vectorized_weights = np.vectorize(self.loss.weight)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        if weights_init is not None:\n",
    "            self.weights_ = weights_init\n",
    "        else:\n",
    "            self.weights_ = 1. / n_samples * np.ones(n_samples)\n",
    "        self.errors_ = [np.inf]\n",
    "        self.n_iterations_ = 0\n",
    "        not_done_yet = True\n",
    "        while not_done_yet:\n",
    "            # Calculating components with current weights\n",
    "            self.mean_ = np.average(X, axis=0, weights=self.weights_)\n",
    "            X_centered = X - self.mean_\n",
    "            U, S, V = linalg.svd(X_centered * np.sqrt(self.weights_.reshape(-1,1)))\n",
    "            # U, V = svd_flip(U, V)\n",
    "            self.components_ = V[:n_components, :]\n",
    "\n",
    "            # Calculate current errors in different models\n",
    "            if self.model == 'first':\n",
    "                non_projected_metric = np.eye(n_features) - \\\n",
    "                        self.components_.T.dot(self.components_)\n",
    "                errors_raw = np.sqrt(np.diag(X_centered.dot(non_projected_metric.dot(X_centered.T))))\n",
    "            elif self.model == 'second':\n",
    "                # Obtain inverse empirical covariance from the SVD\n",
    "                R_inv = np.diag(1. / S**2.)\n",
    "                inverse_cov = V.T.dot(R_inv.dot(V))\n",
    "                errors_raw = np.sqrt(np.diag(X_centered.dot(inverse_cov.dot(X_centered.T))))\n",
    "            else:\n",
    "                raise ValueError('Model should be either \\\"first\\\" or \\\"second\\\".') \n",
    "\n",
    "            errors_loss = vectorized_loss(errors_raw)\n",
    "\n",
    "            # New weights based on errors\n",
    "            self.weights_ = vectorized_weights(errors_raw)\n",
    "            self.weights_ /= self.weights_.sum()\n",
    "            # Checking stopping criteria\n",
    "            self.n_iterations_ += 1\n",
    "            old_total_error = self.errors_[-1]\n",
    "            total_error = errors_loss.sum()\n",
    "\n",
    "            if not np.equal(total_error, 0.):\n",
    "                rel_error = abs(total_error - old_total_error) / abs(total_error)\n",
    "            else:\n",
    "                rel_error = 0.\n",
    "\n",
    "            print('[RPCA] Iteraton %d: error %f, relative error %f'%(self.n_iterations_,\n",
    "                                                                     total_error,\n",
    "                                                                     rel_error))\n",
    "            self.errors_.append(total_error)\n",
    "            not_done_yet = rel_error > self.eps and self.n_iterations_ < self.max_iter\n",
    "        if rel_error > self.eps:\n",
    "            warnings.warn('[RPCA] Did not reach desired precision after %d iterations; relative\\\n",
    "                          error %f instead of specified maximum %f'%(self.n_iterations_,\n",
    "                                                                     rel_error,\n",
    "                                                                     self.eps))\n",
    "        # Get variance explained by singular values\n",
    "        explained_variance_ = (S ** 2) / n_samples\n",
    "        total_var = explained_variance_.sum()\n",
    "        if not np.equal(total_var, 0.):\n",
    "            explained_variance_ratio_ = explained_variance_ / total_var\n",
    "        else:\n",
    "            explained_variance_ratio_ = np.zeros_like(explained_variance_)\n",
    "        self.n_samples_, self.n_features_ = n_samples, n_features\n",
    "        self.n_components_ = n_components\n",
    "        self.explained_variance_ = explained_variance_[:n_components]\n",
    "        self.explained_variance_ratio_ = \\\n",
    "            explained_variance_ratio_[:n_components]\n",
    "\n",
    "        self.errors_ = np.array(self.errors_[1:])\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply dimensionality reduction to X.\n",
    "\n",
    "        X is projected on the first principal components previously extracted\n",
    "        from a training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            New data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array-like, shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        return super(MRobustPCA, self).transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Transform data back to its original space.\n",
    "\n",
    "        In other words, return an input X_original whose transform would be X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_components)\n",
    "            New data, where n_samples is the number of samples\n",
    "            and n_components is the number of components.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_original array-like, shape (n_samples, n_features)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        If whitening is enabled, inverse_transform will compute the\n",
    "        exact inverse operation, which includes reversing whitening.\n",
    "        \"\"\"\n",
    "        return super(MRobustPCA, self).inverse_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER METHOD TO PRINT OUT VALUES CONTAINED IN RPCA MODEL\n",
    "def print_rpca_model(model):\n",
    "    \"\"\"\n",
    "    components\\_ : array, [n_components, n_features]\n",
    "        Principal axes in feature space. The components are sorted by\n",
    "        ``explained_variance_``.\n",
    "\n",
    "    explained_variance\\_ : array, [n_components]\n",
    "        The amount of variance explained by each of the selected components.\n",
    "\n",
    "    explained_variance_ratio\\_ : array, [n_components]\n",
    "        Percentage of variance explained by each of the selected components.\n",
    "        If ``n_components`` is not set then all components are stored and the\n",
    "        sum of explained variances is equal to 1.0.\n",
    "\n",
    "    mean\\_ : array, [n_features]\n",
    "        Per-feature empirical mean, estimated from the training set.\n",
    "        Equal to `X.mean(axis=1)`.\n",
    "\n",
    "    n_components\\_ : int\n",
    "        The number of components. It equals the parameter\n",
    "        n_components, or n_features if n_components is None.    \n",
    "\n",
    "    weights\\_ : array, [n_samples]\n",
    "        Weights assigned to input samples during IRLS optimisation.\n",
    "\n",
    "    n_iterations\\_ : int\n",
    "        Number of iterations performed during fitting.\n",
    "\n",
    "    errors\\_ : array, [n_iterations\\_]\n",
    "        Errors achieved by the method on each iteration.\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'components_'):\n",
    "        print(\"components: %s\" % model.components_)\n",
    "\n",
    "    if hasattr(model, 'explained_variance_'):\n",
    "        print(\"explained_variance: %s\" % model.explained_variance_)\n",
    "\n",
    "    if hasattr(model, 'mean_'):\n",
    "        print(\"mean: %s\" % model.mean_)\n",
    "\n",
    "    if hasattr(model, 'n_components_'):\n",
    "        print(\"n_components: %d\" % model.n_components_)\n",
    "\n",
    "    if hasattr(model, 'weights_'):\n",
    "        print(\"weights: %s\" % model.weights_)\n",
    "\n",
    "    if hasattr(model, 'n_iterations_'):\n",
    "        print(\"n_iterations: %d\" % model.n_iterations_)\n",
    "\n",
    "    if hasattr(model, 'errors_'):\n",
    "        print(\"errors: %s\" % model.errors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD TO COMPUTE ERROR SCORE BETWEEN AN ORIGINAL AND A RECONSTRUCTED DATASET\n",
    "def normalized_anomaly_scores(df_original, df_reconstructed):\n",
    "    \"\"\"\n",
    "    The reconstruction error is the sum of the squared differences between the original and the reconstructed dataset.\n",
    "    The sum of the squared differences is scaled by the max-min range of the sum of the squared differences,\n",
    "    so that all reconstruction errors are within a range of 0 to 1 (normalized). In consequence, normal data\n",
    "    should have a low score whereas anomalies have higher scores.\n",
    "\n",
    "    :param df_original: the original dataset as dataframe\n",
    "    :param df_reconstructed: the reconstructed dataset from rpca\n",
    "    :return: anomaly scores as series with range 0 to 1\n",
    "    \"\"\"\n",
    "\n",
    "    diff = np.sum((np.array(df_original) - np.array(df_reconstructed)) ** 2, axis=1)\n",
    "    diff = pd.Series(data=diff, index=df_original.index)\n",
    "    print(\"score before normalization\")\n",
    "    print(diff)\n",
    "    diff = (diff - np.min(diff)) / (np.max(diff) - np.min(diff))\n",
    "    print(\"\\nscore after normalization\")\n",
    "    print(diff)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER METHODS TO COMPUTE PERFORMANCE METRICS\n",
    "\n",
    "def get_threshold_scores(thresholds, y_scores, valid, upper_boundary=False):\n",
    "    \"\"\"\n",
    "    Computes for each possible threshold the score for the performance metrics\n",
    "\n",
    "    :param thresholds: a list of possible thresholds\n",
    "    :param y_scores: the list of computed scores by the detection algorithm\n",
    "    :param valid: the true class values to run the performance metric against\n",
    "    :param upper_boundary: determines if score higher than thresholds are anomalies or not\n",
    "\n",
    "    :return: array of scores for each threshold for each performance metric\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_hat = np.array(y_scores < threshold).astype(int) if upper_boundary == False else np.array(y_scores > threshold).astype(int)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true=valid['class'].values, y_pred=y_hat)\n",
    "        recall = recall_score(y_true=valid['class'].values, y_pred=y_hat)\n",
    "        precision = precision_score(y_true=valid['class'].values, y_pred=y_hat)\n",
    "        f1_score = fbeta_score(y_true=valid['class'].values, y_pred=y_hat, beta=1)\n",
    "        print('Threshold: %.3f' % threshold)\n",
    "        print('Accuracy Score: %.3f' % accuracy)\n",
    "        print('Recall Score: %.3f' % recall)\n",
    "        print('Precision Score: %.3f' % precision)\n",
    "        print('F1 Score: %.3f' % f1_score)\n",
    "        print(\"\\n\")\n",
    "        scores.append([recall,\n",
    "                       precision,\n",
    "                       f1_score,\n",
    "                       accuracy])\n",
    "\n",
    "    return np.array(scores)\n",
    "\n",
    "def get_max_score_index_for_score_type(threshold_scores, score_type):\n",
    "    if score_type == \"F1_SCORE\":\n",
    "        return threshold_scores[:, 2].argmax()\n",
    "    elif score_type == \"RECALL\":\n",
    "        return threshold_scores[:, 0].argmax()\n",
    "    elif score_type == \"PRECISION\":\n",
    "        return threshold_scores[:, 1].argmax()\n",
    "    elif score_type == \"ACCURACY\":\n",
    "        return threshold_scores[:, 3].argmax()\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature_1  feature_2  feature_3\n",
      "time                                       \n",
      "2020-05-13          4          6        100\n",
      "2020-05-14          7          5          2\n"
     ]
    }
   ],
   "source": [
    "# MAIN DATA\n",
    "date_now = dt.datetime.now()\n",
    "date_today = date_now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "days = pd.date_range(date_today, date_today + dt.timedelta(1), freq='D')\n",
    "\n",
    "np.random.seed(seed=1234)\n",
    "data_1 = np.random.randint(1, high=10, size=len(days))\n",
    "data_2 = np.random.randint(1, high=10, size=len(days))\n",
    "data_3 = np.array([100, 2])\n",
    "df = pd.DataFrame({'time': days, 'feature_1': data_1, 'feature_2': data_2, 'feature_3': data_3 })\n",
    "df = df.set_index('time')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            class\n",
      "time             \n",
      "2020-05-13   True\n",
      "2020-05-14  False\n"
     ]
    }
   ],
   "source": [
    "# MAIN DATA CLASS\n",
    "\"\"\"\n",
    "    Row with index 0 in df is outlier, as it contains a high value out of bound (1, 10)\n",
    "\"\"\"\n",
    "data_df_class = np.array([True, False])\n",
    "df_class = pd.DataFrame({'time': days, 'class': data_df_class })\n",
    "df_class = df_class.set_index('time')\n",
    "\n",
    "print(df_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature_1  feature_2  feature_3\n",
      "time                                       \n",
      "2020-05-13          3          3          8\n",
      "2020-05-14          9        100          6\n",
      "2020-05-15          3          6          3\n",
      "2020-05-16          2        100          5\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA\n",
    "date_now = dt.datetime.now()\n",
    "date_today = date_now.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "days = pd.date_range(date_today, date_today + dt.timedelta(3), freq='D')\n",
    "\n",
    "np.random.seed(seed=4321)\n",
    "data_1 = np.random.randint(1, high=10, size=len(days))\n",
    "data_2 = np.array([3, 100, 6, 100])\n",
    "data_3 = np.random.randint(1, high=10, size=len(days))\n",
    "df_train = pd.DataFrame({'time': days, 'feature_1': data_1, 'feature_2': data_2, 'feature_3': data_3 })\n",
    "df_train = df_train.set_index('time')\n",
    "\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            class\n",
      "time             \n",
      "2020-05-13  False\n",
      "2020-05-14   True\n",
      "2020-05-15  False\n",
      "2020-05-16   True\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA CLASS\n",
    "\"\"\"\n",
    "    Rows with index 1 and 3 in df_train are outliers, as they contain a high value out of bound (1, 10)\n",
    "\"\"\"\n",
    "data_train_class = np.array([False, True, False, True])\n",
    "df_train_class = pd.DataFrame({'time': days, 'class': data_train_class })\n",
    "df_train_class = df_train_class.set_index('time')\n",
    "\n",
    "print(df_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== X_TRAIN ===\n",
      "            feature_1  feature_2  feature_3\n",
      "time                                       \n",
      "2020-05-14          9        100          6\n",
      "2020-05-13          3          3          8\n",
      "\n",
      "\n",
      "=== y_train ===\n",
      "            class\n",
      "time             \n",
      "2020-05-14   True\n",
      "2020-05-13  False\n",
      "\n",
      "\n",
      "=== X_test ===\n",
      "            feature_1  feature_2  feature_3\n",
      "time                                       \n",
      "2020-05-15          3          6          3\n",
      "2020-05-16          2        100          5\n",
      "\n",
      "\n",
      "=== y_test ===\n",
      "            class\n",
      "time             \n",
      "2020-05-15  False\n",
      "2020-05-16   True\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA SPLIT INTO TRAIN AND VALIDATION PARTS\n",
    "\"\"\"\n",
    "    stratify parameter makes a split so that the proportion of values in the sample produced\n",
    "    will be the same as the proportion of values provided to parameter stratify\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train, df_train_class, train_size=0.5, random_state=10, stratify=df_train_class)\n",
    "\n",
    "print(\"=== X_TRAIN ===\")\n",
    "print(X_train)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=== y_train ===\")\n",
    "print(y_train)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=== X_test ===\")\n",
    "print(X_test)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"=== y_test ===\")\n",
    "print(y_test)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE MODEL\n",
    "\"\"\"\n",
    "    Dimensionality reduction using Robust PCA and Huber Loss Function\n",
    "    \n",
    "    :param delta: delta value (=threshold) for Huber Loss Function\n",
    "    :param n_components: the number of principal components for dimensionality reduction\n",
    "\"\"\"\n",
    "huber_loss = HuberLoss(delta=1)\n",
    "M_rpca = MRobustPCA(1, huber_loss)\n",
    "\n",
    "# We see that M_rpca is only initialized, but hasn't computed anything yet\n",
    "print_rpca_model(M_rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RPCA] Iteraton 1: error 0.000000, relative error inf\n",
      "[RPCA] Iteraton 2: error 0.000000, relative error 0.000000\n",
      "components: [[-0.06172461 -0.99788113  0.02057487]]\n",
      "explained_variance: [1181.125]\n",
      "mean: [ 6.  51.5  7. ]\n",
      "n_components: 1\n",
      "weights: [0.5 0.5]\n",
      "n_iterations: 2\n",
      "errors: [2.50893924e-13 2.50893924e-13]\n"
     ]
    }
   ],
   "source": [
    "# FIT MODEL ON TRAINING (X_train)\n",
    "M_rpca.fit(X_train)\n",
    "\n",
    "print_rpca_model(M_rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== X_test_reduced ===\n",
      "                    0\n",
      "time                 \n",
      "2020-05-15  45.506466\n",
      "2020-05-16 -48.191486\n",
      "\n",
      "RPCA MODEL\n",
      "components: [[-0.06172461 -0.99788113  0.02057487]]\n",
      "explained_variance: [1181.125]\n",
      "mean: [ 6.  51.5  7. ]\n",
      "n_components: 1\n",
      "weights: [0.5 0.5]\n",
      "n_iterations: 2\n",
      "errors: [2.50893924e-13 2.50893924e-13]\n"
     ]
    }
   ],
   "source": [
    "# REDUCE TEST SET WITH FITTED MODEL\n",
    "X_test_reduced = M_rpca.transform(X_test)\n",
    "X_test_reduced = pd.DataFrame(data=X_test_reduced, index=X_test.index)\n",
    "\n",
    "print(\"=== X_test_reduced ===\")\n",
    "print(X_test_reduced)\n",
    "\n",
    "# NOTE THE MODEL HAS STILL SAME COMPONENTS AND ATTRIBUTES\n",
    "print(\"\\nRPCA MODEL\")\n",
    "print_rpca_model(M_rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== X_test_reconstructed ===\n",
      "                   0          1         2\n",
      "time                                     \n",
      "2020-05-15  3.191131   6.089957  7.936290\n",
      "2020-05-16  8.974600  99.589375  6.008467\n",
      "\n",
      "=== X_test - original for reference - ===\n",
      "            feature_1  feature_2  feature_3\n",
      "time                                       \n",
      "2020-05-15          3          6          3\n",
      "2020-05-16          2        100          5\n",
      "\n",
      "RPCA MODEL\n",
      "components: [[-0.06172461 -0.99788113  0.02057487]]\n",
      "explained_variance: [1181.125]\n",
      "mean: [ 6.  51.5  7. ]\n",
      "n_components: 1\n",
      "weights: [0.5 0.5]\n",
      "n_iterations: 2\n",
      "errors: [2.50893924e-13 2.50893924e-13]\n"
     ]
    }
   ],
   "source": [
    "# RECONSTRUCT TEST SET WITH FITTED MODEL\n",
    "X_test_reconstructed = M_rpca.inverse_transform(X_test_reduced)\n",
    "X_test_reconstructed = pd.DataFrame(data=X_test_reconstructed, index=X_test.index)\n",
    "\n",
    "print(\"=== X_test_reconstructed ===\")\n",
    "print(X_test_reconstructed)\n",
    "print(\"\\n=== X_test - original for reference - ===\")\n",
    "print(X_test)\n",
    "\n",
    "# NOTE THE MODEL HAS STILL SAME COMPONENTS AND ATTRIBUTES\n",
    "print(\"\\nRPCA MODEL\")\n",
    "print_rpca_model(M_rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score before normalization\n",
      "time\n",
      "2020-05-15    24.411578\n",
      "2020-05-16    49.830670\n",
      "dtype: float64\n",
      "\n",
      "score after normalization\n",
      "time\n",
      "2020-05-15    0.0\n",
      "2020-05-16    1.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "time\n",
      "2020-05-15    0.0\n",
      "2020-05-16    1.0\n",
      "dtype: float64\n",
      "              0  class\n",
      "time                  \n",
      "2020-05-15  0.0  False\n",
      "2020-05-16  1.0   True\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE SCORE FROM RECONSTRUCTION ERROR\n",
    "\"\"\"\n",
    "    The larger error score the more likley it's an outlier\n",
    "\"\"\"\n",
    "y_test_scores = normalized_anomaly_scores(X_test, X_test_reconstructed)\n",
    "print(y_test_scores)\n",
    "\n",
    "# COMBINE WITH TRUTH\n",
    "y_test_scores_class = y_test_scores.to_frame().join(y_test)\n",
    "print(y_test_scores_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible anomaly score thresholds\n",
      "[0.         0.33333333 0.66666667 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE ANOMALY SCORE THRESHOLD\n",
    "\"\"\"\n",
    "    we want to determine an threshold for anomaly detection. If a computed score is above the threshold \n",
    "    it will be marked as anomaly, otherwise it will marked as normal data\n",
    "    \n",
    "    for simplicity we use range (0,1) and only 4 candidates\n",
    "\"\"\"\n",
    "lower_bound = 0\n",
    "higher_bound = 1\n",
    "thresholds = np.linspace(lower_bound, higher_bound, 4) # => 4 candidates\n",
    "\n",
    "print(\"possible anomaly score thresholds\")\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.000\n",
      "Accuracy Score: 1.000\n",
      "Recall Score: 1.000\n",
      "Precision Score: 1.000\n",
      "F1 Score: 1.000\n",
      "\n",
      "\n",
      "Threshold: 0.333\n",
      "Accuracy Score: 1.000\n",
      "Recall Score: 1.000\n",
      "Precision Score: 1.000\n",
      "F1 Score: 1.000\n",
      "\n",
      "\n",
      "Threshold: 0.667\n",
      "Accuracy Score: 1.000\n",
      "Recall Score: 1.000\n",
      "Precision Score: 1.000\n",
      "F1 Score: 1.000\n",
      "\n",
      "\n",
      "Threshold: 1.000\n",
      "Accuracy Score: 0.500\n",
      "Recall Score: 0.000\n",
      "Precision Score: 0.000\n",
      "F1 Score: 0.000\n",
      "\n",
      "\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/Vadetis/venv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# For all possible thresholds we compute the performance metrics, \n",
    "# and select the final threshold for maximizing F1 Score\n",
    "threshold_scores = get_threshold_scores(thresholds, y_test_scores, y_test, upper_boundary=True)\n",
    "selected_index = get_max_score_index_for_score_type(threshold_scores, \"F1_SCORE\")\n",
    "selected_threshold = thresholds[selected_index]\n",
    "\n",
    "# Now we have our final threshold 0.0, which means anything with score > 0.0 will be marked as anomaly\n",
    "print(selected_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            feature_1  feature_2  feature_3\n",
      "time                                       \n",
      "2020-05-13          4          6        100\n",
      "2020-05-14          7          5          2\n",
      "\n",
      "RPCA MODEL\n",
      "components: [[-0.06172461 -0.99788113  0.02057487]]\n",
      "explained_variance: [1181.125]\n",
      "mean: [ 6.  51.5  7. ]\n",
      "n_components: 1\n",
      "weights: [0.5 0.5]\n",
      "n_iterations: 2\n",
      "errors: [2.50893924e-13 2.50893924e-13]\n"
     ]
    }
   ],
   "source": [
    "# NOW LETS COMPUTE ANOMALY SCORES FOR OUR MAIN DATASET\n",
    "# NOTE THE MODEL HAS STILL SAME COMPONENTS AND ATTRIBUTES\n",
    "print(df)\n",
    "print(\"\\nRPCA MODEL\")\n",
    "print_rpca_model(M_rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== X_df_reduced ===\n",
      "                    0\n",
      "time                 \n",
      "2020-05-13  47.440503\n",
      "2020-05-14  46.236874\n",
      "\n",
      "RPCA MODEL\n",
      "components: [[-0.06172461 -0.99788113  0.02057487]]\n",
      "explained_variance: [1181.125]\n",
      "mean: [ 6.  51.5  7. ]\n",
      "n_components: 1\n",
      "weights: [0.5 0.5]\n",
      "n_iterations: 2\n",
      "errors: [2.50893924e-13 2.50893924e-13]\n"
     ]
    }
   ],
   "source": [
    "# REDUCE MAIN DATASET WITH FITTED MODEL\n",
    "X_df_reduced = M_rpca.transform(df)\n",
    "X_df_reduced = pd.DataFrame(data=X_df_reduced, index=df.index)\n",
    "\n",
    "print(\"=== X_df_reduced ===\")\n",
    "print(X_df_reduced)\n",
    "\n",
    "# NOTE THE MODEL HAS STILL SAME COMPONENTS AND ATTRIBUTES\n",
    "print(\"\\nRPCA MODEL\")\n",
    "print_rpca_model(M_rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== X_df_reconstructed ===\n",
      "                   0         1         2\n",
      "time                                    \n",
      "2020-05-13  3.071754  4.160017  7.976082\n",
      "2020-05-14  3.146047  5.361096  7.951318\n",
      "\n",
      "=== df - original for reference - ===\n",
      "            feature_1  feature_2  feature_3\n",
      "time                                       \n",
      "2020-05-13          4          6        100\n",
      "2020-05-14          7          5          2\n",
      "\n",
      "RPCA MODEL\n",
      "components: [[-0.06172461 -0.99788113  0.02057487]]\n",
      "explained_variance: [1181.125]\n",
      "mean: [ 6.  51.5  7. ]\n",
      "n_components: 1\n",
      "weights: [0.5 0.5]\n",
      "n_iterations: 2\n",
      "errors: [2.50893924e-13 2.50893924e-13]\n"
     ]
    }
   ],
   "source": [
    "# RECONSTRUCT MAIN SET WITH FITTED MODEL\n",
    "X_df_reconstructed = M_rpca.inverse_transform(X_df_reduced)\n",
    "X_df_reconstructed = pd.DataFrame(data=X_df_reconstructed, index=df.index)\n",
    "\n",
    "print(\"=== X_df_reconstructed ===\")\n",
    "print(X_df_reconstructed)\n",
    "print(\"\\n=== df - original for reference - ===\")\n",
    "print(df)\n",
    "\n",
    "# NOTE THE MODEL HAS STILL SAME COMPONENTS AND ATTRIBUTES\n",
    "print(\"\\nRPCA MODEL\")\n",
    "print_rpca_model(M_rpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score before normalization\n",
      "time\n",
      "2020-05-13    8472.648640\n",
      "2020-05-14      50.401524\n",
      "dtype: float64\n",
      "\n",
      "score after normalization\n",
      "time\n",
      "2020-05-13    1.0\n",
      "2020-05-14    0.0\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "scores\n",
      "time\n",
      "2020-05-13    1.0\n",
      "2020-05-14    0.0\n",
      "dtype: float64\n",
      "\n",
      "detection results\n",
      "time\n",
      "2020-05-13    1\n",
      "2020-05-14    0\n",
      "dtype: int64\n",
      "\n",
      "Comparision - Detection <-> Truth\n",
      "            0  class\n",
      "time                \n",
      "2020-05-13  1   True\n",
      "2020-05-14  0  False\n",
      "\n",
      "Threshold: 0.000\n",
      "Accuracy Score: 1.000\n",
      "Recall Score: 1.000\n",
      "Precision Score: 1.000\n",
      "F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# COMPUTE ANOMALY SCORE \n",
    "scores = normalized_anomaly_scores(df, X_df_reconstructed)\n",
    "print(\"scores\")\n",
    "print(scores)\n",
    "\n",
    "y_hat_results = (scores > selected_threshold).astype(int)\n",
    "print(\"\\ndetection results\")\n",
    "print(y_hat_results)\n",
    "\n",
    "# COMPARE TO TRUTH\n",
    "y_truth = df_class.values.astype(int)\n",
    "print(\"\\nComparision - Detection <-> Truth\")\n",
    "print(y_hat_results.to_frame().join(df_class))\n",
    "\n",
    "accuracy = accuracy_score(y_pred=y_hat_results, y_true=y_truth)\n",
    "recall = recall_score(y_pred=y_hat_results, y_true=y_truth)\n",
    "precision = precision_score(y_pred=y_hat_results, y_true=y_truth)\n",
    "f1_score = fbeta_score(y_pred=y_hat_results, y_true=y_truth, beta=1)\n",
    "\n",
    "print('\\nThreshold: %.3f' % selected_threshold)\n",
    "print('Accuracy Score: %.3f' % accuracy)\n",
    "print('Recall Score: %.3f' % recall)\n",
    "print('Precision Score: %.3f' % precision)\n",
    "print('F1 Score: %.3f' % f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
